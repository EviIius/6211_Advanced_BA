{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c511293-b583-4488-ae5e-ef8e70c620a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Mining - Federalist Papers\n",
    "# Part 4\n",
    "# We will combine all text pre-processing into a single cell and proceed from there\n",
    "# SVD and predicting authorship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503edfe1-09ce-4c86-9975-f594321260e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import key libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4f97f8-2dcc-41d0-9ab8-c10a5cf7d286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset is in Documents folder, so changing default folder to Documents before reading\n",
    "# import os and change directory to Documents\n",
    "import os\n",
    "# os.chdir(\"Documents\")\n",
    "\n",
    "# read federalist.csv\n",
    "papers = pd.read_csv(\"federalist.csv\")\n",
    "papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad449b8-ac9d-4b7a-a793-2e9e6db19e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining all pre-processing into a single cell\n",
    "# filter to papers written by Hamilton, Madison, and Unknown\n",
    "papers = papers[papers[\"Author\"].isin([\"HAMILTON\", \"MADISON\",\"UNKNOWN\"])]\n",
    "\n",
    "# remove the common first sentence from all documents\n",
    "papers[\"Text\"] = papers[\"Text\"].str.replace(\"To the People of the State of New York:\", \"\")\n",
    "\n",
    "# Remove punctuation from the text column\n",
    "papers[\"Text\"] = papers[\"Text\"].str.replace('[^\\w\\s]', '', regex=True)\n",
    "\n",
    "# convert all words to lowercase\n",
    "papers[\"Text\"] = papers[\"Text\"].str.lower()\n",
    "\n",
    "# removal of stop_words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stop = stopwords.words(\"english\")\n",
    "\n",
    "papers[\"Text\"] = papers[\"Text\"].apply(lambda x: \" \".join(x for x in x.split()\n",
    "                                                         if x not in stop))\n",
    "# stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()\n",
    "papers[\"Text\"] = papers[\"Text\"].apply(lambda x: \" \".join([st.stem(word)\n",
    "                                                         for word in x.split()]))\n",
    "\n",
    "# further remove custom stopwords, which are problem specific\n",
    "stop += [\"would\", \"may\", \"must\", \"one\", \"upon\", \"might\", \"shall\", \"could\"]\n",
    "papers[\"Text\"] = papers[\"Text\"].apply(lambda x: \" \".join(x for x in x.split()\n",
    "                                                         if x not in stop))\n",
    "\n",
    "papers[\"Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6a6145-dcf7-49dc-8f4d-a044a7b34935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the corpus and dictionary\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Tokenize the documents in the Text column\n",
    "corpus = [doc.split() for doc in papers[\"Text\"]]\n",
    "\n",
    "# Create the term dictionary of the corpus\n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "\n",
    "dictionary.filter_extremes(no_below = 2, no_above = 0.75)\n",
    "\n",
    "# Convert the corpus into Document Term Matrix\n",
    "DFM = [dictionary.doc2bow(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5424184f-7d41-42be-8fbb-8b71e04f877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SVD to create 8 dimensions\n",
    "# https://radimrehurek.com/gensim/models/lsimodel.html\n",
    "# create the TF-IDF model\n",
    "tfidf = models.TfidfModel(DFM)\n",
    "DFM_tfidf = tfidf[DFM]\n",
    "\n",
    "n_SVD = 8\n",
    "SVD_model = models.LsiModel(DFM_tfidf, id2word=dictionary, num_topics=n_SVD)\n",
    "SVD = SVD_model[DFM_tfidf]\n",
    "\n",
    "# convert results into array\n",
    "svd_array = gensim.matutils.corpus2csc(SVD).T.toarray()\n",
    "\n",
    "# convert results to data frame\n",
    "svd_df = pd.DataFrame(svd_array)\n",
    "\n",
    "# show SVD results - reduced vector representation of the documents\n",
    "svd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361fdf44-82d6-4f64-a22d-240210062197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data frame for predictive models\n",
    "model_df = pd.concat([papers.reset_index()[\"Author\"], svd_df], axis=1, ignore_index=True).rename({0:\"Author\"}, axis=1)\n",
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbff18e-2cea-4074-9caf-9b67d4dbbab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# partitioning data so test data's authors are UNKNOWN\n",
    "# no validation set due to small sample size\n",
    "testData = model_df[model_df[\"Author\"] == \"UNKNOWN\"]\n",
    "trainData = model_df[~(model_df[\"Author\"] == \"UNKNOWN\")]\n",
    "\n",
    "# manually dummy code target variable so Author HAMILTON = 1\n",
    "trainData[\"Author\"] = [1 if x == \"HAMILTON\" else 0 for x in trainData.Author]\n",
    "\n",
    "# create DV and IV sets\n",
    "y_train = trainData[\"Author\"]\n",
    "X_train = trainData[trainData.columns[trainData.columns != \"Author\"]]\n",
    "X_test = testData[testData.columns[testData.columns != \"Author\"]]\n",
    "\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3c8267-1e4e-4a08-b50d-7ecd2185ed03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build decision tree predictive model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "tree.fit(X_train, y_train)\n",
    "tree_predictions = tree.predict(X_test)\n",
    "tree_predictions\n",
    "\n",
    "tree.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c57c683-139a-4297-be6b-cd820a4bac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression(class_weight=\"balanced\")\n",
    "LR.fit(X_train, y_train)\n",
    "\n",
    "lr_predictions = LR.predict(X_test)\n",
    "lr_predictions\n",
    "\n",
    "LR.score(X_train, y_train)\n",
    "\n",
    "results = pd.DataFrame(lr_predictions, columns=[\"Sklearn LR\"])\n",
    "results[\"Decision Tree\"] = tree_predictions\n",
    "\n",
    "doc_labels = list(range(65,77))\n",
    "results.index = doc_labels\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7645e2ae-4af1-450b-8e85-52bdc2332143",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
